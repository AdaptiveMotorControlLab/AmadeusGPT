{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bceb3204-2a87-4671-8135-2533a7a51771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amadeusgpt.main import AMADEUS\n",
    "from amadeusgpt.config import Config\n",
    "import amadeusgpt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be76dc87-fbe8-452f-b85c-2af3e95a03bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoint_info:\n",
      "  body_orientation_keypoints:\n",
      "    animal_center: neck\n",
      "    neck: nose\n",
      "    tail_base: tail base\n",
      "  head_orientation_keypoints:\n",
      "    neck: neck\n",
      "    nose: nose\n",
      "  keypoint_file_path: null\n",
      "  model_checkpoint: ''\n",
      "  model_name: ''\n",
      "llm_info:\n",
      "  keep_last_n_messages: 2\n",
      "object_info:\n",
      "  load_objects_from_disk: false\n",
      "video_info:\n",
      "  scene_frame_number: 100\n",
      "  video_file_path: /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current total cost 0.0 $\n",
      "current total tokens 540\n",
      "The image shows a person walking a horse on a dirt path. The background includes trees, a large tent-like structure, and other buildings or structures. The scene appears to be outdoors, possibly at a park or a fairground.\n",
      "\n",
      "Here is the JSON string based on the description:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"description\": \"A person walking a horse on a dirt path with trees, a tent-like structure, and other buildings in the background.\",\n",
      "    \"individuals\": 1,\n",
      "    \"species\": \"sideview_quadruped\",\n",
      "    \"background_objects\": [\"trees\", \"tent-like structure\", \"buildings\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "amadeus_root = Path(amadeusgpt.__file__).parent.parent\n",
    "config = Config( amadeus_root / \"amadeusgpt/configs/Horse_template.yaml\")\n",
    "config['video_info']['video_file_path'] = str(amadeus_root / config['video_info']['video_file_path'])\n",
    "config['keypoint_info']['keypoint_file_path'] = None #str(amadeus_root / config['keypoint_info']['keypoint_file_path'])\n",
    "print (config)\n",
    "amadeus = AMADEUS(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59edc48e-cc37-4c42-a7f0-a0c7f6d3cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def parse_result(amadeus, qa_message):\n",
    "    display(Markdown(qa_message['chain_of_thought']))\n",
    "    sandbox = amadeus.sandbox\n",
    "    qa_message = sandbox.code_execution(qa_message)\n",
    "    sandbox.render_qa_message(qa_message)\n",
    "    print ('after executing the function')\n",
    "    display(qa_message['meta_info'])\n",
    "    display(Markdown(str(qa_message['function_rets'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28b3f10-ecba-4ecf-a283-142d2d43ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current total cost 0.02 $\n",
      "current total tokens 3274\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To plot the gait analysis results, we need to follow these steps:\n",
       "\n",
       "1. Run the gait analysis using the provided keypoints.\n",
       "2. Plot the results of the gait analysis.\n",
       "\n",
       "We can achieve this using the `run_gait_analysis` and `plot_gait_analysis_results` functions from the `AnimalBehaviorAnalysis` class.\n",
       "\n",
       "Here is the code to accomplish this:\n",
       "\n",
       "```python\n",
       "def plot_gait_analysis_results(config: Config):\n",
       "    '''\n",
       "    Parameters:\n",
       "    ----------\n",
       "    config: Config\n",
       "    '''\n",
       "    # create_analysis returns an instance of AnimalBehaviorAnalysis\n",
       "    analysis = create_analysis(config)\n",
       "    \n",
       "    # Define the limb keypoints\n",
       "    limb_keypoints = [\"Offfrontfoot\", \"Offfrontfetlock\", \"Offknee\", \"Elbow\", \"Shoulder\"]\n",
       "    \n",
       "    # Run gait analysis\n",
       "    gait_analysis_results = analysis.run_gait_analysis(limb_keypoint_names=limb_keypoints)\n",
       "    \n",
       "    # Plot the gait analysis results\n",
       "    figure, axs = analysis.plot_gait_analysis_results(gait_analysis_results=gait_analysis_results, \n",
       "                                                      limb_keypoints=limb_keypoints, \n",
       "                                                      color_stance=\"plum\")\n",
       "    return figure, axs\n",
       "```\n",
       "\n",
       "This function will:\n",
       "1. Create an instance of `AnimalBehaviorAnalysis`.\n",
       "2. Define the limb keypoints as specified.\n",
       "3. Run the gait analysis using these keypoints.\n",
       "4. Plot the results of the gait analysis and return the figure and axes objects for further manipulation or display."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc1...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n",
      "going to inference video with superanimal_quadruped_hrnetw32\n",
      "running video inference on ['/mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4'] with superanimal_quadruped_hrnetw32\n",
      "Using pytorch for model hrnetw32\n",
      "using /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4 for video adaptation training\n",
      "Task: None\n",
      "scorer: None\n",
      "date: None\n",
      "multianimalproject: None\n",
      "identity: None\n",
      "project_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/project_configs\n",
      "engine: pytorch\n",
      "video_sets: None\n",
      "bodyparts: ['nose', 'upper_jaw', 'lower_jaw', 'mouth_end_right', 'mouth_end_left', 'right_eye', 'right_earbase', 'right_earend', 'right_antler_base', 'right_antler_end', 'left_eye', 'left_earbase', 'left_earend', 'left_antler_base', 'left_antler_end', 'neck_base', 'neck_end', 'throat_base', 'throat_end', 'back_base', 'back_end', 'back_middle', 'tail_base', 'tail_end', 'front_left_thai', 'front_left_knee', 'front_left_paw', 'front_right_thai', 'front_right_knee', 'front_right_paw', 'back_left_paw', 'back_left_thai', 'back_right_thai', 'back_left_knee', 'back_right_knee', 'back_right_paw', 'belly_bottom', 'body_middle_right', 'body_middle_left']\n",
      "start: None\n",
      "stop: None\n",
      "numframes2pick: None\n",
      "skeleton: []\n",
      "skeleton_color: black\n",
      "pcutoff: None\n",
      "dotsize: None\n",
      "alphavalue: None\n",
      "colormap: rainbow\n",
      "TrainingFraction: None\n",
      "iteration: None\n",
      "default_net_type: None\n",
      "default_augmenter: None\n",
      "snapshotindex: None\n",
      "detector_snapshotindex: None\n",
      "batch_size: 1\n",
      "cropping: None\n",
      "x1: None\n",
      "x2: None\n",
      "y1: None\n",
      "y2: None\n",
      "corner2move2: None\n",
      "move2corner: None\n",
      "SuperAnimalConversionTables: None\n",
      "data:\n",
      "  colormode: RGB\n",
      "  inference:\n",
      "    auto_padding:\n",
      "      pad_width_divisor: 32\n",
      "      pad_height_divisor: 32\n",
      "    normalize_images: True\n",
      "  train:\n",
      "    affine:\n",
      "      p: 0.5\n",
      "      scaling: [1.0, 1.0]\n",
      "      rotation: 30\n",
      "      translation: 0\n",
      "    gaussian_noise: 12.75\n",
      "    normalize_images: True\n",
      "    auto_padding:\n",
      "      pad_width_divisor: 32\n",
      "      pad_height_divisor: 32\n",
      "detector:\n",
      "  data:\n",
      "    colormode: RGB\n",
      "    inference:\n",
      "      normalize_images: True\n",
      "    train:\n",
      "      hflip: True\n",
      "      normalize_images: True\n",
      "  device: auto\n",
      "  model:\n",
      "    type: FasterRCNN\n",
      "    variant: fasterrcnn_resnet50_fpn_v2\n",
      "    box_score_thresh: 0.6\n",
      "    pretrained: False\n",
      "  runner:\n",
      "    type: DetectorTrainingRunner\n",
      "    eval_interval: 50\n",
      "    optimizer:\n",
      "      type: AdamW\n",
      "      params:\n",
      "        lr: 1e-05\n",
      "    scheduler:\n",
      "      type: LRListScheduler\n",
      "      params:\n",
      "        milestones: [90]\n",
      "        lr_list: [[1e-06]]\n",
      "    snapshots:\n",
      "      max_snapshots: 5\n",
      "      save_epochs: 50\n",
      "      save_optimizer_state: False\n",
      "  train_settings:\n",
      "    batch_size: 1\n",
      "    dataloader_workers: 0\n",
      "    dataloader_pin_memory: True\n",
      "    display_iters: 500\n",
      "    epochs: 250\n",
      "device: auto\n",
      "method: td\n",
      "model:\n",
      "  backbone:\n",
      "    type: HRNet\n",
      "    model_name: hrnet_w32\n",
      "    pretrained: False\n",
      "    freeze_bn_stats: True\n",
      "    freeze_bn_weights: False\n",
      "    interpolate_branches: False\n",
      "    increased_channel_count: False\n",
      "  backbone_output_channels: 32\n",
      "  heads:\n",
      "    bodypart:\n",
      "      type: HeatmapHead\n",
      "      weight_init: normal\n",
      "      predictor:\n",
      "        type: HeatmapPredictor\n",
      "        apply_sigmoid: False\n",
      "        clip_scores: True\n",
      "        location_refinement: False\n",
      "        locref_std: 7.2801\n",
      "      target_generator:\n",
      "        type: HeatmapGaussianGenerator\n",
      "        num_heatmaps: 39\n",
      "        pos_dist_thresh: 17\n",
      "        heatmap_mode: KEYPOINT\n",
      "        generate_locref: False\n",
      "        locref_std: 7.2801\n",
      "      criterion:\n",
      "        heatmap:\n",
      "          type: WeightedMSECriterion\n",
      "          weight: 1.0\n",
      "      heatmap_config:\n",
      "        channels: [32, 39]\n",
      "        kernel_size: [1]\n",
      "        strides: [1]\n",
      "runner:\n",
      "  type: PoseTrainingRunner\n",
      "  key_metric: test.mAP\n",
      "  key_metric_asc: True\n",
      "  eval_interval: 10\n",
      "  optimizer:\n",
      "    type: AdamW\n",
      "    params:\n",
      "      lr: 1e-05\n",
      "  scheduler:\n",
      "    type: LRListScheduler\n",
      "    params:\n",
      "      lr_list: [[1e-06], [1e-07]]\n",
      "      milestones: [160, 190]\n",
      "  snapshots:\n",
      "    max_snapshots: 5\n",
      "    save_epochs: 25\n",
      "    save_optimizer_state: False\n",
      "train_settings:\n",
      "  batch_size: 1\n",
      "  dataloader_workers: 0\n",
      "  dataloader_pin_memory: True\n",
      "  display_iters: 500\n",
      "  epochs: 200\n",
      "  pretrained_weights: None\n",
      "  seed: 42\n",
      "metadata:\n",
      "  project_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/project_configs\n",
      "  pose_config_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/model_configs/hrnetw32.yaml\n",
      "  bodyparts: ['nose', 'upper_jaw', 'lower_jaw', 'mouth_end_right', 'mouth_end_left', 'right_eye', 'right_earbase', 'right_earend', 'right_antler_base', 'right_antler_end', 'left_eye', 'left_earbase', 'left_earend', 'left_antler_base', 'left_antler_end', 'neck_base', 'neck_end', 'throat_base', 'throat_end', 'back_base', 'back_end', 'back_middle', 'tail_base', 'tail_end', 'front_left_thai', 'front_left_knee', 'front_left_paw', 'front_right_thai', 'front_right_knee', 'front_right_paw', 'back_left_paw', 'back_left_thai', 'back_right_thai', 'back_left_knee', 'back_right_knee', 'back_right_paw', 'belly_bottom', 'body_middle_right', 'body_middle_left']\n",
      "  unique_bodyparts: []\n",
      "  individuals: ['animal']\n",
      "  with_identity: None\n",
      "Processing video /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4\n",
      "Starting to analyze /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    308\n",
      "  Duration of video [s]:  12.83\n",
      "  fps:                    24.0\n",
      "  resolution:             w=288, h=162\n",
      "\n",
      "Running Detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 308/308 [00:26<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Pose Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 308/308 [00:08<00:00, 34.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse\n",
      "Saving results in /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow_superanimal_quadruped_hrnetw32.h5 and /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow_superanimal_quadruped_hrnetw32_full.pickle\n",
      "Duration of video [s]: 12.83, recorded with 24.0 fps!\n",
      "Overall # of frames: 308 with cropped frame dimensions: 288 162\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 308/308 [00:00<00:00, 365.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with predictions was saved as /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse\n",
      "Task: None\n",
      "scorer: None\n",
      "date: None\n",
      "multianimalproject: None\n",
      "identity: None\n",
      "project_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/project_configs\n",
      "engine: pytorch\n",
      "video_sets: None\n",
      "bodyparts: ['nose', 'upper_jaw', 'lower_jaw', 'mouth_end_right', 'mouth_end_left', 'right_eye', 'right_earbase', 'right_earend', 'right_antler_base', 'right_antler_end', 'left_eye', 'left_earbase', 'left_earend', 'left_antler_base', 'left_antler_end', 'neck_base', 'neck_end', 'throat_base', 'throat_end', 'back_base', 'back_end', 'back_middle', 'tail_base', 'tail_end', 'front_left_thai', 'front_left_knee', 'front_left_paw', 'front_right_thai', 'front_right_knee', 'front_right_paw', 'back_left_paw', 'back_left_thai', 'back_right_thai', 'back_left_knee', 'back_right_knee', 'back_right_paw', 'belly_bottom', 'body_middle_right', 'body_middle_left']\n",
      "start: None\n",
      "stop: None\n",
      "numframes2pick: None\n",
      "skeleton: []\n",
      "skeleton_color: black\n",
      "pcutoff: None\n",
      "dotsize: None\n",
      "alphavalue: None\n",
      "colormap: rainbow\n",
      "TrainingFraction: None\n",
      "iteration: None\n",
      "default_net_type: None\n",
      "default_augmenter: None\n",
      "snapshotindex: None\n",
      "detector_snapshotindex: None\n",
      "batch_size: 1\n",
      "cropping: None\n",
      "x1: None\n",
      "x2: None\n",
      "y1: None\n",
      "y2: None\n",
      "corner2move2: None\n",
      "move2corner: None\n",
      "SuperAnimalConversionTables: None\n",
      "data:\n",
      "  colormode: RGB\n",
      "  inference:\n",
      "    auto_padding:\n",
      "      pad_width_divisor: 32\n",
      "      pad_height_divisor: 32\n",
      "    normalize_images: True\n",
      "  train:\n",
      "    affine:\n",
      "      p: 0.5\n",
      "      scaling: [1.0, 1.0]\n",
      "      rotation: 30\n",
      "      translation: 0\n",
      "    gaussian_noise: 12.75\n",
      "    normalize_images: True\n",
      "    auto_padding:\n",
      "      pad_width_divisor: 32\n",
      "      pad_height_divisor: 32\n",
      "detector:\n",
      "  data:\n",
      "    colormode: RGB\n",
      "    inference:\n",
      "      normalize_images: True\n",
      "    train:\n",
      "      hflip: True\n",
      "      normalize_images: True\n",
      "  device: auto\n",
      "  model:\n",
      "    type: FasterRCNN\n",
      "    variant: fasterrcnn_resnet50_fpn_v2\n",
      "    box_score_thresh: 0.6\n",
      "    pretrained: False\n",
      "  runner:\n",
      "    type: DetectorTrainingRunner\n",
      "    eval_interval: 50\n",
      "    optimizer:\n",
      "      type: AdamW\n",
      "      params:\n",
      "        lr: 1e-05\n",
      "    scheduler:\n",
      "      type: LRListScheduler\n",
      "      params:\n",
      "        milestones: [90]\n",
      "        lr_list: [[1e-06]]\n",
      "    snapshots:\n",
      "      max_snapshots: 5\n",
      "      save_epochs: 50\n",
      "      save_optimizer_state: False\n",
      "  train_settings:\n",
      "    batch_size: 1\n",
      "    dataloader_workers: 0\n",
      "    dataloader_pin_memory: True\n",
      "    display_iters: 500\n",
      "    epochs: 250\n",
      "device: auto\n",
      "method: td\n",
      "model:\n",
      "  backbone:\n",
      "    type: HRNet\n",
      "    model_name: hrnet_w32\n",
      "    pretrained: False\n",
      "    freeze_bn_stats: True\n",
      "    freeze_bn_weights: False\n",
      "    interpolate_branches: False\n",
      "    increased_channel_count: False\n",
      "  backbone_output_channels: 32\n",
      "  heads:\n",
      "    bodypart:\n",
      "      type: HeatmapHead\n",
      "      weight_init: normal\n",
      "      predictor:\n",
      "        type: HeatmapPredictor\n",
      "        apply_sigmoid: False\n",
      "        clip_scores: True\n",
      "        location_refinement: False\n",
      "        locref_std: 7.2801\n",
      "      target_generator:\n",
      "        type: HeatmapGaussianGenerator\n",
      "        num_heatmaps: 39\n",
      "        pos_dist_thresh: 17\n",
      "        heatmap_mode: KEYPOINT\n",
      "        generate_locref: False\n",
      "        locref_std: 7.2801\n",
      "      criterion:\n",
      "        heatmap:\n",
      "          type: WeightedMSECriterion\n",
      "          weight: 1.0\n",
      "      heatmap_config:\n",
      "        channels: [32, 39]\n",
      "        kernel_size: [1]\n",
      "        strides: [1]\n",
      "runner:\n",
      "  type: PoseTrainingRunner\n",
      "  key_metric: test.mAP\n",
      "  key_metric_asc: True\n",
      "  eval_interval: 10\n",
      "  optimizer:\n",
      "    type: AdamW\n",
      "    params:\n",
      "      lr: 1e-05\n",
      "  scheduler:\n",
      "    type: LRListScheduler\n",
      "    params:\n",
      "      lr_list: [[1e-06], [1e-07]]\n",
      "      milestones: [160, 190]\n",
      "  snapshots:\n",
      "    max_snapshots: 5\n",
      "    save_epochs: 25\n",
      "    save_optimizer_state: False\n",
      "train_settings:\n",
      "  batch_size: 1\n",
      "  dataloader_workers: 0\n",
      "  dataloader_pin_memory: True\n",
      "  display_iters: 500\n",
      "  epochs: 200\n",
      "  pretrained_weights: None\n",
      "  seed: 42\n",
      "metadata:\n",
      "  project_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/project_configs\n",
      "  pose_config_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/model_configs/hrnetw32.yaml\n",
      "  bodyparts: ['nose', 'upper_jaw', 'lower_jaw', 'mouth_end_right', 'mouth_end_left', 'right_eye', 'right_earbase', 'right_earend', 'right_antler_base', 'right_antler_end', 'left_eye', 'left_earbase', 'left_earend', 'left_antler_base', 'left_antler_end', 'neck_base', 'neck_end', 'throat_base', 'throat_end', 'back_base', 'back_end', 'back_middle', 'tail_base', 'tail_end', 'front_left_thai', 'front_left_knee', 'front_left_paw', 'front_right_thai', 'front_right_knee', 'front_right_paw', 'back_left_paw', 'back_left_thai', 'back_right_thai', 'back_left_knee', 'back_right_knee', 'back_right_paw', 'belly_bottom', 'body_middle_right', 'body_middle_left']\n",
      "  unique_bodyparts: []\n",
      "  individuals: ['animal']\n",
      "  with_identity: None\n",
      "Video frames being extracted to /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/pseudo_BrownHorseinShadow/images for video adaptation.\n",
      "Constructing pseudo dataset at /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/pseudo_BrownHorseinShadow\n",
      "\n",
      "Running video adaptation with following parameters: \n",
      "(pose training) pose_epochs: 4\n",
      "(pose) save_epochs: 1\n",
      "detector_epochs: 4\n",
      "detector_save_epochs: 1\n",
      "video adaptation batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /home/shaokai/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 167M/167M [00:01<00:00, 115MB/s]\n",
      "Data Transforms:\n",
      "  Training:   Compose([\n",
      "  HorizontalFlip(always_apply=False, p=0.5),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
      "  Validation: Compose([\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
      "\n",
      "Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.\n",
      "This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.\n",
      "If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. \n",
      "This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).\n",
      "\n",
      "Using 26 images and 10 for testing\n",
      "\n",
      "Starting object detector training...\n",
      "--------------------------------------------------\n",
      "Epoch 1/4 (lr=1e-05), train loss 0.05062\n",
      "Epoch 2/4 (lr=1e-05), train loss 0.04007\n",
      "Epoch 3/4 (lr=1e-05), train loss 0.02521\n",
      "Epoch 4/4 (lr=1e-05), train loss 0.02673\n",
      "Loading pretrained weights from Hugging Face hub (timm/hrnet_w32.ms_in1k)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0acaf964e874031bcb8b5a3eae0275c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/165M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[timm/hrnet_w32.ms_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Unexpected keys (downsamp_modules.0.0.bias, downsamp_modules.0.0.weight, downsamp_modules.0.1.bias, downsamp_modules.0.1.num_batches_tracked, downsamp_modules.0.1.running_mean, downsamp_modules.0.1.running_var, downsamp_modules.0.1.weight, downsamp_modules.1.0.bias, downsamp_modules.1.0.weight, downsamp_modules.1.1.bias, downsamp_modules.1.1.num_batches_tracked, downsamp_modules.1.1.running_mean, downsamp_modules.1.1.running_var, downsamp_modules.1.1.weight, downsamp_modules.2.0.bias, downsamp_modules.2.0.weight, downsamp_modules.2.1.bias, downsamp_modules.2.1.num_batches_tracked, downsamp_modules.2.1.running_mean, downsamp_modules.2.1.running_var, downsamp_modules.2.1.weight, final_layer.0.bias, final_layer.0.weight, final_layer.1.bias, final_layer.1.num_batches_tracked, final_layer.1.running_mean, final_layer.1.running_var, final_layer.1.weight, incre_modules.0.0.bn1.bias, incre_modules.0.0.bn1.num_batches_tracked, incre_modules.0.0.bn1.running_mean, incre_modules.0.0.bn1.running_var, incre_modules.0.0.bn1.weight, incre_modules.0.0.bn2.bias, incre_modules.0.0.bn2.num_batches_tracked, incre_modules.0.0.bn2.running_mean, incre_modules.0.0.bn2.running_var, incre_modules.0.0.bn2.weight, incre_modules.0.0.bn3.bias, incre_modules.0.0.bn3.num_batches_tracked, incre_modules.0.0.bn3.running_mean, incre_modules.0.0.bn3.running_var, incre_modules.0.0.bn3.weight, incre_modules.0.0.conv1.weight, incre_modules.0.0.conv2.weight, incre_modules.0.0.conv3.weight, incre_modules.0.0.downsample.0.weight, incre_modules.0.0.downsample.1.bias, incre_modules.0.0.downsample.1.num_batches_tracked, incre_modules.0.0.downsample.1.running_mean, incre_modules.0.0.downsample.1.running_var, incre_modules.0.0.downsample.1.weight, incre_modules.1.0.bn1.bias, incre_modules.1.0.bn1.num_batches_tracked, incre_modules.1.0.bn1.running_mean, incre_modules.1.0.bn1.running_var, incre_modules.1.0.bn1.weight, incre_modules.1.0.bn2.bias, incre_modules.1.0.bn2.num_batches_tracked, incre_modules.1.0.bn2.running_mean, incre_modules.1.0.bn2.running_var, incre_modules.1.0.bn2.weight, incre_modules.1.0.bn3.bias, incre_modules.1.0.bn3.num_batches_tracked, incre_modules.1.0.bn3.running_mean, incre_modules.1.0.bn3.running_var, incre_modules.1.0.bn3.weight, incre_modules.1.0.conv1.weight, incre_modules.1.0.conv2.weight, incre_modules.1.0.conv3.weight, incre_modules.1.0.downsample.0.weight, incre_modules.1.0.downsample.1.bias, incre_modules.1.0.downsample.1.num_batches_tracked, incre_modules.1.0.downsample.1.running_mean, incre_modules.1.0.downsample.1.running_var, incre_modules.1.0.downsample.1.weight, incre_modules.2.0.bn1.bias, incre_modules.2.0.bn1.num_batches_tracked, incre_modules.2.0.bn1.running_mean, incre_modules.2.0.bn1.running_var, incre_modules.2.0.bn1.weight, incre_modules.2.0.bn2.bias, incre_modules.2.0.bn2.num_batches_tracked, incre_modules.2.0.bn2.running_mean, incre_modules.2.0.bn2.running_var, incre_modules.2.0.bn2.weight, incre_modules.2.0.bn3.bias, incre_modules.2.0.bn3.num_batches_tracked, incre_modules.2.0.bn3.running_mean, incre_modules.2.0.bn3.running_var, incre_modules.2.0.bn3.weight, incre_modules.2.0.conv1.weight, incre_modules.2.0.conv2.weight, incre_modules.2.0.conv3.weight, incre_modules.2.0.downsample.0.weight, incre_modules.2.0.downsample.1.bias, incre_modules.2.0.downsample.1.num_batches_tracked, incre_modules.2.0.downsample.1.running_mean, incre_modules.2.0.downsample.1.running_var, incre_modules.2.0.downsample.1.weight, incre_modules.3.0.bn1.bias, incre_modules.3.0.bn1.num_batches_tracked, incre_modules.3.0.bn1.running_mean, incre_modules.3.0.bn1.running_var, incre_modules.3.0.bn1.weight, incre_modules.3.0.bn2.bias, incre_modules.3.0.bn2.num_batches_tracked, incre_modules.3.0.bn2.running_mean, incre_modules.3.0.bn2.running_var, incre_modules.3.0.bn2.weight, incre_modules.3.0.bn3.bias, incre_modules.3.0.bn3.num_batches_tracked, incre_modules.3.0.bn3.running_mean, incre_modules.3.0.bn3.running_var, incre_modules.3.0.bn3.weight, incre_modules.3.0.conv1.weight, incre_modules.3.0.conv2.weight, incre_modules.3.0.conv3.weight, incre_modules.3.0.downsample.0.weight, incre_modules.3.0.downsample.1.bias, incre_modules.3.0.downsample.1.num_batches_tracked, incre_modules.3.0.downsample.1.running_mean, incre_modules.3.0.downsample.1.running_var, incre_modules.3.0.downsample.1.weight, classifier.bias, classifier.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "Data Transforms:\n",
      "  Training:   Compose([\n",
      "  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n",
      "  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),\n",
      "  PadIfNeeded(always_apply=False, p=1.0, min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, position=PositionType.RANDOM, border_mode=4, value=None, mask_value=None),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
      "  Validation: Compose([\n",
      "  PadIfNeeded(always_apply=False, p=1.0, min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, position=PositionType.RANDOM, border_mode=4, value=None, mask_value=None),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
      "\n",
      "Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.\n",
      "This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.\n",
      "If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. \n",
      "This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).\n",
      "\n",
      "Using 26 images and 10 for testing\n",
      "\n",
      "Starting pose model training...\n",
      "--------------------------------------------------\n",
      "Training for epoch 1 done, starting evaluation\n",
      "Epoch 1 performance:\n",
      "metrics/test.rmse:  28.369\n",
      "metrics/test.rmse_pcutoff:0.743\n",
      "metrics/test.mAP:   36.463\n",
      "metrics/test.mAR:   49.000\n",
      "metrics/test.mAP_pcutoff:0.000\n",
      "metrics/test.mAR_pcutoff:0.000\n",
      "Epoch 1/4 (lr=1e-05), train loss 0.00299, valid loss 0.00189\n",
      "Training for epoch 2 done, starting evaluation\n",
      "Epoch 2 performance:\n",
      "metrics/test.rmse:  28.254\n",
      "metrics/test.rmse_pcutoff:1.149\n",
      "metrics/test.mAP:   37.564\n",
      "metrics/test.mAR:   50.000\n",
      "metrics/test.mAP_pcutoff:0.000\n",
      "metrics/test.mAR_pcutoff:0.000\n",
      "Epoch 2/4 (lr=1e-05), train loss 0.00244, valid loss 0.00179\n",
      "Training for epoch 3 done, starting evaluation\n",
      "Epoch 3 performance:\n",
      "metrics/test.rmse:  28.014\n",
      "metrics/test.rmse_pcutoff:1.144\n",
      "metrics/test.mAP:   40.822\n",
      "metrics/test.mAR:   52.000\n",
      "metrics/test.mAP_pcutoff:2.079\n",
      "metrics/test.mAR_pcutoff:2.000\n",
      "Epoch 3/4 (lr=1e-05), train loss 0.00236, valid loss 0.00170\n",
      "Training for epoch 4 done, starting evaluation\n",
      "Epoch 4 performance:\n",
      "metrics/test.rmse:  28.287\n",
      "metrics/test.rmse_pcutoff:1.222\n",
      "metrics/test.mAP:   39.129\n",
      "metrics/test.mAR:   51.000\n",
      "metrics/test.mAP_pcutoff:2.079\n",
      "metrics/test.mAR_pcutoff:2.000\n",
      "Epoch 4/4 (lr=1e-05), train loss 0.00212, valid loss 0.00161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: None\n",
      "scorer: None\n",
      "date: None\n",
      "multianimalproject: None\n",
      "identity: None\n",
      "project_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/project_configs\n",
      "engine: pytorch\n",
      "video_sets: None\n",
      "bodyparts: ['nose', 'upper_jaw', 'lower_jaw', 'mouth_end_right', 'mouth_end_left', 'right_eye', 'right_earbase', 'right_earend', 'right_antler_base', 'right_antler_end', 'left_eye', 'left_earbase', 'left_earend', 'left_antler_base', 'left_antler_end', 'neck_base', 'neck_end', 'throat_base', 'throat_end', 'back_base', 'back_end', 'back_middle', 'tail_base', 'tail_end', 'front_left_thai', 'front_left_knee', 'front_left_paw', 'front_right_thai', 'front_right_knee', 'front_right_paw', 'back_left_paw', 'back_left_thai', 'back_right_thai', 'back_left_knee', 'back_right_knee', 'back_right_paw', 'belly_bottom', 'body_middle_right', 'body_middle_left']\n",
      "start: None\n",
      "stop: None\n",
      "numframes2pick: None\n",
      "skeleton: []\n",
      "skeleton_color: black\n",
      "pcutoff: None\n",
      "dotsize: None\n",
      "alphavalue: None\n",
      "colormap: rainbow\n",
      "TrainingFraction: None\n",
      "iteration: None\n",
      "default_net_type: None\n",
      "default_augmenter: None\n",
      "snapshotindex: None\n",
      "detector_snapshotindex: None\n",
      "batch_size: 1\n",
      "cropping: None\n",
      "x1: None\n",
      "x2: None\n",
      "y1: None\n",
      "y2: None\n",
      "corner2move2: None\n",
      "move2corner: None\n",
      "SuperAnimalConversionTables: None\n",
      "data:\n",
      "  colormode: RGB\n",
      "  inference:\n",
      "    auto_padding:\n",
      "      pad_width_divisor: 32\n",
      "      pad_height_divisor: 32\n",
      "    normalize_images: True\n",
      "  train:\n",
      "    affine:\n",
      "      p: 0.5\n",
      "      scaling: [1.0, 1.0]\n",
      "      rotation: 30\n",
      "      translation: 0\n",
      "    gaussian_noise: 12.75\n",
      "    normalize_images: True\n",
      "    auto_padding:\n",
      "      pad_width_divisor: 32\n",
      "      pad_height_divisor: 32\n",
      "detector:\n",
      "  data:\n",
      "    colormode: RGB\n",
      "    inference:\n",
      "      normalize_images: True\n",
      "    train:\n",
      "      hflip: True\n",
      "      normalize_images: True\n",
      "  device: auto\n",
      "  model:\n",
      "    type: FasterRCNN\n",
      "    variant: fasterrcnn_resnet50_fpn_v2\n",
      "    box_score_thresh: 0.6\n",
      "    pretrained: False\n",
      "  runner:\n",
      "    type: DetectorTrainingRunner\n",
      "    eval_interval: 50\n",
      "    optimizer:\n",
      "      type: AdamW\n",
      "      params:\n",
      "        lr: 1e-05\n",
      "    scheduler:\n",
      "      type: LRListScheduler\n",
      "      params:\n",
      "        milestones: [90]\n",
      "        lr_list: [[1e-06]]\n",
      "    snapshots:\n",
      "      max_snapshots: 5\n",
      "      save_epochs: 50\n",
      "      save_optimizer_state: False\n",
      "  train_settings:\n",
      "    batch_size: 1\n",
      "    dataloader_workers: 0\n",
      "    dataloader_pin_memory: True\n",
      "    display_iters: 500\n",
      "    epochs: 250\n",
      "device: auto\n",
      "method: td\n",
      "model:\n",
      "  backbone:\n",
      "    type: HRNet\n",
      "    model_name: hrnet_w32\n",
      "    pretrained: False\n",
      "    freeze_bn_stats: True\n",
      "    freeze_bn_weights: False\n",
      "    interpolate_branches: False\n",
      "    increased_channel_count: False\n",
      "  backbone_output_channels: 32\n",
      "  heads:\n",
      "    bodypart:\n",
      "      type: HeatmapHead\n",
      "      weight_init: normal\n",
      "      predictor:\n",
      "        type: HeatmapPredictor\n",
      "        apply_sigmoid: False\n",
      "        clip_scores: True\n",
      "        location_refinement: False\n",
      "        locref_std: 7.2801\n",
      "      target_generator:\n",
      "        type: HeatmapGaussianGenerator\n",
      "        num_heatmaps: 39\n",
      "        pos_dist_thresh: 17\n",
      "        heatmap_mode: KEYPOINT\n",
      "        generate_locref: False\n",
      "        locref_std: 7.2801\n",
      "      criterion:\n",
      "        heatmap:\n",
      "          type: WeightedMSECriterion\n",
      "          weight: 1.0\n",
      "      heatmap_config:\n",
      "        channels: [32, 39]\n",
      "        kernel_size: [1]\n",
      "        strides: [1]\n",
      "runner:\n",
      "  type: PoseTrainingRunner\n",
      "  key_metric: test.mAP\n",
      "  key_metric_asc: True\n",
      "  eval_interval: 10\n",
      "  optimizer:\n",
      "    type: AdamW\n",
      "    params:\n",
      "      lr: 1e-05\n",
      "  scheduler:\n",
      "    type: LRListScheduler\n",
      "    params:\n",
      "      lr_list: [[1e-06], [1e-07]]\n",
      "      milestones: [160, 190]\n",
      "  snapshots:\n",
      "    max_snapshots: 5\n",
      "    save_epochs: 25\n",
      "    save_optimizer_state: False\n",
      "train_settings:\n",
      "  batch_size: 1\n",
      "  dataloader_workers: 0\n",
      "  dataloader_pin_memory: True\n",
      "  display_iters: 500\n",
      "  epochs: 200\n",
      "  pretrained_weights: None\n",
      "  seed: 42\n",
      "metadata:\n",
      "  project_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/project_configs\n",
      "  pose_config_path: /mnt/md0/shaokai/miniconda3/envs/amadeusgpt-gpu/lib/python3.10/site-packages/deeplabcut/modelzoo/model_configs/hrnetw32.yaml\n",
      "  bodyparts: ['nose', 'upper_jaw', 'lower_jaw', 'mouth_end_right', 'mouth_end_left', 'right_eye', 'right_earbase', 'right_earend', 'right_antler_base', 'right_antler_end', 'left_eye', 'left_earbase', 'left_earend', 'left_antler_base', 'left_antler_end', 'neck_base', 'neck_end', 'throat_base', 'throat_end', 'back_base', 'back_end', 'back_middle', 'tail_base', 'tail_end', 'front_left_thai', 'front_left_knee', 'front_left_paw', 'front_right_thai', 'front_right_knee', 'front_right_paw', 'back_left_paw', 'back_left_thai', 'back_right_thai', 'back_left_knee', 'back_right_knee', 'back_right_paw', 'belly_bottom', 'body_middle_right', 'body_middle_left']\n",
      "  unique_bodyparts: []\n",
      "  individuals: ['animal']\n",
      "  with_identity: None\n",
      "Processing video /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4\n",
      "Starting to analyze /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    308\n",
      "  Duration of video [s]:  12.83\n",
      "  fps:                    24.0\n",
      "  resolution:             w=288, h=162\n",
      "\n",
      "Running Detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 308/308 [00:24<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Pose Prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 308/308 [00:09<00:00, 31.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse\n",
      "Saving results in /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow_superanimal_quadruped_hrnetw32.h5 and /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse/BrownHorseinShadow_superanimal_quadruped_hrnetw32_full.pickle\n",
      "Duration of video [s]: 12.83, recorded with 24.0 fps!\n",
      "Overall # of frames: 308 with cropped frame dimensions: 288 162\n",
      "Generating frames and creating video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 308/308 [00:00<00:00, 325.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with predictions was saved as /mnt/md0/shaokai/AmadeusGPT-dev/examples/Horse\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/md0/shaokai/AmadeusGPT-dev/amadeusgpt/programs/sandbox.py\", line 340, in code_execution\n",
      "    exec(f\"result = {call_str}\", self.exec_namespace)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 15, in plot_gait_analysis_results\n",
      "  File \"/mnt/md0/shaokai/AmadeusGPT-dev/amadeusgpt/integration_modules/kinematics/gait.py\", line 111, in run_gait_analysis\n",
      "    contacts = get_events(self, \"contact\", limb_keypoint_names, min_dist)\n",
      "  File \"/mnt/md0/shaokai/AmadeusGPT-dev/amadeusgpt/integration_modules/kinematics/gait.py\", line 43, in get_events\n",
      "    x = coords[:, 0, analysis.get_keypoint_names().index(kpt), 0]\n",
      "ValueError: 'Offfrontfoot' is not in list\n",
      "\n",
      "after executing the function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'A person walking a horse on a dirt path with trees, a tent-like structure, and other buildings in the background.',\n",
       " 'individuals': 1,\n",
       " 'species': 'sideview_quadruped',\n",
       " 'background_objects': ['trees', 'tent-like structure', 'buildings']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "None"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Plot me the gait analysis results. The limb is defined with the following keypoints: front_left_thai, front_left_knee\"\n",
    "qa_message = amadeus.step(query)\n",
    "parse_result(amadeus, qa_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea38d91-3bd1-448c-b395-9524891e6567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7406a-2417-488e-a196-429e34ab18b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amadeusgpt-gpu",
   "language": "python",
   "name": "amadeusgpt-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
